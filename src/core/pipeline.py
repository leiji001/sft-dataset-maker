"""æ ¸å¿ƒå¤„ç†æµæ°´çº¿

æµç¨‹: ç”¨æˆ·è¾“å…¥æ–‡ä»¶ â†’ æ–‡æ¡£æå– â†’ é—®é¢˜åˆ›å»º â†’ é—®é¢˜å›ç­” â†’ è¾“å‡ºSFTæ•°æ®é›†

å¯¹åº”æµç¨‹å›¾:
  ç”¨æˆ·è¾“å…¥ (file) â†’ æ–‡æ¡£æå–å™¨ â†’ é—®é¢˜åˆ›å»º (deepseek-chat)
  â†’ é—®é¢˜å›ç­” (deepseek-chat) â†’ è¾“å‡º (é—®é¢˜åˆ›å»º text + é—®é¢˜å›ç­” text)
"""

import json
import re
from dataclasses import dataclass
from pathlib import Path

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

from src.config import AppConfig
from src.document_parser import DocumentParser
from src.llm import LLMClient

console = Console()


@dataclass
class QAPair:
    """é—®ç­”å¯¹"""

    question: str
    answer: str
    source_chunk: str = ""


@dataclass
class SFTSample:
    """SFT è®­ç»ƒæ ·æœ¬"""

    instruction: str
    input: str
    output: str
    source_file: str = ""


class TextChunker:
    """æ–‡æœ¬åˆ†å—å™¨"""

    def __init__(self, chunk_size: int = 2000, chunk_overlap: int = 200) -> None:
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split(self, text: str) -> list[str]:
        """å°†é•¿æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ªå—

        æŒ‰æ®µè½è¾¹ç•Œåˆ†å‰², å°½å¯èƒ½ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
        """
        if len(text) <= self.chunk_size:
            return [text.strip()] if text.strip() else []

        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r"\n{2,}", text)
        chunks: list[str] = []
        current_chunk: list[str] = []
        current_length = 0

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            para_len = len(para)

            # å•ä¸ªæ®µè½è¶…è¿‡ chunk_size, éœ€è¦å¼ºåˆ¶åˆ†å‰²
            if para_len > self.chunk_size:
                # å…ˆä¿å­˜å½“å‰ç§¯ç´¯çš„å†…å®¹
                if current_chunk:
                    chunks.append("\n\n".join(current_chunk))
                    current_chunk = []
                    current_length = 0

                # æŒ‰å­—ç¬¦æ•°å¼ºåˆ¶åˆ†å‰²é•¿æ®µè½
                for i in range(0, para_len, self.chunk_size - self.chunk_overlap):
                    sub = para[i : i + self.chunk_size]
                    if sub.strip():
                        chunks.append(sub.strip())
                continue

            # å¦‚æœåŠ å…¥å½“å‰æ®µè½ä¼šè¶…é™, å…ˆä¿å­˜
            if current_length + para_len + 2 > self.chunk_size and current_chunk:
                chunks.append("\n\n".join(current_chunk))
                # ä¿ç•™é‡å éƒ¨åˆ†
                overlap_text = "\n\n".join(current_chunk)
                if len(overlap_text) > self.chunk_overlap:
                    # å–æœ€åè‹¥å¹²æ®µè½ä½œä¸ºé‡å 
                    overlap_parts: list[str] = []
                    overlap_len = 0
                    for p in reversed(current_chunk):
                        if overlap_len + len(p) > self.chunk_overlap:
                            break
                        overlap_parts.insert(0, p)
                        overlap_len += len(p) + 2
                    current_chunk = overlap_parts
                    current_length = sum(len(p) + 2 for p in current_chunk)
                else:
                    current_chunk = []
                    current_length = 0

            current_chunk.append(para)
            current_length += para_len + 2

        # ä¿å­˜æœ€åä¸€å—
        if current_chunk:
            chunks.append("\n\n".join(current_chunk))

        return chunks


def parse_questions(questions_text: str) -> list[str]:
    """ä» LLM è¾“å‡ºä¸­è§£æé—®é¢˜åˆ—è¡¨"""
    questions: list[str] = []
    for line in questions_text.strip().splitlines():
        line = line.strip()
        if not line:
            continue
        # å»é™¤å¸¸è§çš„åºå·å‰ç¼€: Q1: / 1. / 1ã€/ 1) ç­‰
        cleaned = re.sub(r"^(Q?\d+[\.:ï¼šã€\)ï¼‰]\s*)", "", line)
        if cleaned:
            questions.append(cleaned)
    return questions


class SFTPipeline:
    """SFT æ•°æ®é›†åˆ¶ä½œæµæ°´çº¿

    å®Œæ•´æµç¨‹:
    1. æ–‡æ¡£è§£æ â†’ æå–æ–‡æœ¬
    2. æ–‡æœ¬åˆ†å— â†’ åˆ‡åˆ†ä¸ºé€‚åˆ LLM å¤„ç†çš„ç‰‡æ®µ
    3. é—®é¢˜åˆ›å»º â†’ LLM åŸºäºæ¯ä¸ªæ–‡æœ¬å—ç”Ÿæˆé—®é¢˜
    4. é—®é¢˜å›ç­” â†’ LLM åŸºäºæ–‡æœ¬å—å›ç­”æ¯ä¸ªé—®é¢˜
    5. è¾“å‡º â†’ ç”Ÿæˆ JSONL/JSON æ ¼å¼çš„ SFT æ•°æ®é›†
    """

    def __init__(self, config: AppConfig) -> None:
        self.config = config
        self.doc_parser = DocumentParser(config.mineru)
        self.llm_client = LLMClient(config.llm)
        self.chunker = TextChunker(
            chunk_size=config.process.chunk_size,
            chunk_overlap=config.process.chunk_overlap,
        )

    def process_file(self, file_path: str | Path) -> list[SFTSample]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶, ç”Ÿæˆ SFT è®­ç»ƒæ ·æœ¬

        Args:
            file_path: è¾“å…¥æ–‡ä»¶è·¯å¾„

        Returns:
            ç”Ÿæˆçš„ SFT æ ·æœ¬åˆ—è¡¨
        """
        file_path = Path(file_path).resolve()
        console.rule(f"[bold blue]å¤„ç†æ–‡ä»¶: {file_path.name}")

        # ===== ç¬¬1æ­¥: æ–‡æ¡£æå– =====
        console.print("\n[bold]ğŸ“„ ç¬¬1æ­¥: æ–‡æ¡£æå–[/bold]")
        document_text = self.doc_parser.parse(file_path)
        if not document_text.strip():
            console.print("[red]æ–‡æ¡£å†…å®¹ä¸ºç©º, è·³è¿‡[/red]")
            return []

        # ===== ç¬¬2æ­¥: æ–‡æœ¬åˆ†å— =====
        console.print("\n[bold]âœ‚ï¸  ç¬¬2æ­¥: æ–‡æœ¬åˆ†å—[/bold]")
        chunks = self.chunker.split(document_text)
        console.print(f"å…±åˆ†ä¸º [cyan]{len(chunks)}[/cyan] ä¸ªæ–‡æœ¬å—")

        # ===== ç¬¬3æ­¥ & ç¬¬4æ­¥: é—®é¢˜åˆ›å»º & é—®é¢˜å›ç­” =====
        all_samples: list[SFTSample] = []

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task("å¤„ç†æ–‡æœ¬å—...", total=len(chunks))

            for chunk_idx, chunk in enumerate(chunks, 1):
                progress.update(
                    task, description=f"å¤„ç†æ–‡æœ¬å— {chunk_idx}/{len(chunks)}..."
                )

                # ç¬¬3æ­¥: é—®é¢˜åˆ›å»º
                try:
                    questions_text = self.llm_client.generate_questions(
                        chunk,
                        num_questions=self.config.process.questions_per_chunk,
                    )
                    questions = parse_questions(questions_text)
                except Exception as e:
                    console.print(
                        f"[red]æ–‡æœ¬å— {chunk_idx} ç”Ÿæˆé—®é¢˜å¤±è´¥: {e}[/red]"
                    )
                    progress.advance(task)
                    continue

                if not questions:
                    console.print(
                        f"[yellow]æ–‡æœ¬å— {chunk_idx} æœªèƒ½è§£æå‡ºé—®é¢˜, è·³è¿‡[/yellow]"
                    )
                    progress.advance(task)
                    continue

                # ç¬¬4æ­¥: é—®é¢˜å›ç­”
                for q_idx, question in enumerate(questions, 1):
                    try:
                        answer = self.llm_client.answer_question(chunk, question)
                        sample = SFTSample(
                            instruction=question,
                            input="",
                            output=answer,
                            source_file=str(file_path.name),
                        )
                        all_samples.append(sample)
                    except Exception as e:
                        console.print(
                            f"[red]  é—®é¢˜ {q_idx} å›ç­”å¤±è´¥: {e}[/red]"
                        )

                progress.advance(task)

        console.print(
            f"\n[green]âœ… æ–‡ä»¶å¤„ç†å®Œæˆ, å…±ç”Ÿæˆ {len(all_samples)} æ¡è®­ç»ƒæ ·æœ¬[/green]"
        )
        return all_samples

    def process_directory(self, dir_path: str | Path) -> list[SFTSample]:
        """å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶

        Args:
            dir_path: ç›®å½•è·¯å¾„

        Returns:
            æ‰€æœ‰æ–‡ä»¶ç”Ÿæˆçš„ SFT æ ·æœ¬åˆ—è¡¨
        """
        dir_path = Path(dir_path).resolve()
        if not dir_path.is_dir():
            raise NotADirectoryError(f"ä¸æ˜¯æœ‰æ•ˆçš„ç›®å½•: {dir_path}")

        all_samples: list[SFTSample] = []
        supported_files: list[Path] = []

        for ext in DocumentParser.SUPPORTED_EXTENSIONS:
            supported_files.extend(dir_path.rglob(f"*{ext}"))

        supported_files.sort()

        if not supported_files:
            console.print("[yellow]ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶[/yellow]")
            return []

        console.print(
            f"æ‰¾åˆ° [cyan]{len(supported_files)}[/cyan] ä¸ªå¾…å¤„ç†æ–‡ä»¶\n"
        )

        for file_path in supported_files:
            try:
                samples = self.process_file(file_path)
                all_samples.extend(samples)
            except Exception as e:
                console.print(f"[red]å¤„ç†æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}[/red]")

        return all_samples

    def save_dataset(self, samples: list[SFTSample], output_path: str | Path | None = None) -> Path:
        """ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶

        Args:
            samples: SFT æ ·æœ¬åˆ—è¡¨
            output_path: è¾“å‡ºè·¯å¾„ (å¯é€‰, é»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ output_dir)

        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if output_path is None:
            output_dir = Path(self.config.process.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

            fmt = self.config.process.output_format.lower()
            if fmt == "jsonl":
                output_path = output_dir / "sft_dataset.jsonl"
            else:
                output_path = output_dir / "sft_dataset.json"
        else:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        data = [
            {
                "instruction": s.instruction,
                "input": s.input,
                "output": s.output,
                "source_file": s.source_file,
            }
            for s in samples
        ]

        fmt = output_path.suffix.lstrip(".")

        if fmt == "jsonl":
            with open(output_path, "w", encoding="utf-8") as f:
                for item in data:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
        else:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

        console.print(f"\n[bold green]ğŸ’¾ æ•°æ®é›†å·²ä¿å­˜: {output_path}[/bold green]")
        console.print(f"   å…± [cyan]{len(data)}[/cyan] æ¡è®­ç»ƒæ ·æœ¬")

        return output_path
